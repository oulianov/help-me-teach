{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haxxor/projects/teacher/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should look for activities that promote interdisciplinary learning\n",
      "Action: Google Search about education\n",
      "Action Input: activities to promote interdisciplinary learning\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSelect at most 3 most meaningful keywords separated by a comma from this text : activities to promote interdisciplinary learning\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "['Interdisciplinary', 'Learning', 'Activities']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 140\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39m# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m agent \u001b[39m=\u001b[39m initialize_agent(tools, llm, agent\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mzero-shot-react-description\u001b[39m\u001b[39m\"\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 140\u001b[0m agent\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\u001b[39;49m\u001b[39mquelle activité organiser pour favoriser apprentissage trandisciplinaire ?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/projects/teacher/lib/python3.8/site-packages/langchain/chains/base.py:172\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    171\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m])[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    174\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    175\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m~/projects/teacher/lib/python3.8/site-packages/langchain/chains/base.py:146\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n\u001b[1;32m    145\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 146\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n\u001b[1;32m    148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n",
      "File \u001b[0;32m~/projects/teacher/lib/python3.8/site-packages/langchain/chains/base.py:142\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    139\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m}, inputs\n\u001b[1;32m    140\u001b[0m     )\n\u001b[1;32m    141\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs)\n\u001b[1;32m    143\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    144\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n",
      "File \u001b[0;32m~/projects/teacher/lib/python3.8/site-packages/langchain/agents/agent.py:287\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n\u001b[1;32m    286\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_tool_error(e)\n\u001b[0;32m--> 287\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    288\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    289\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n",
      "File \u001b[0;32m~/projects/teacher/lib/python3.8/site-packages/langchain/agents/agent.py:281\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_tool_start(\n\u001b[1;32m    277\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39m(tool\u001b[39m.\u001b[39mfunc)[:\u001b[39m60\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m}, output, color\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgreen\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     )\n\u001b[1;32m    279\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    280\u001b[0m     \u001b[39m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m     observation \u001b[39m=\u001b[39m tool\u001b[39m.\u001b[39;49mfunc(output\u001b[39m.\u001b[39;49mtool_input)\n\u001b[1;32m    282\u001b[0m     color \u001b[39m=\u001b[39m color_mapping[output\u001b[39m.\u001b[39mtool]\n\u001b[1;32m    283\u001b[0m     return_direct \u001b[39m=\u001b[39m tool\u001b[39m.\u001b[39mreturn_direct\n",
      "Cell \u001b[0;32mIn[1], line 112\u001b[0m, in \u001b[0;36mgoogle_search_about_education\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(snippets_with_keyword) \u001b[39m<\u001b[39m min_snippets: \n\u001b[1;32m    111\u001b[0m     all_snippets \u001b[39m=\u001b[39m text_splitter\u001b[39m.\u001b[39msplit_text(document_text)\n\u001b[0;32m--> 112\u001b[0m     random_snippets \u001b[39m=\u001b[39m sample(all_snippets, max_vector_store \u001b[39m-\u001b[39;49m \u001b[39mlen\u001b[39;49m(snippets_with_keyword))\n\u001b[1;32m    113\u001b[0m     vectorstore \u001b[39m=\u001b[39m FAISS\u001b[39m.\u001b[39mfrom_texts(snippets_with_keyword, embeddings)\n\u001b[1;32m    114\u001b[0m     relevant_snippets \u001b[39m=\u001b[39m text_splitter\u001b[39m.\u001b[39mcreate_documents(snippets_with_keyword) \n",
      "File \u001b[0;32m/usr/lib/python3.8/random.py:363\u001b[0m, in \u001b[0;36mRandom.sample\u001b[0;34m(self, population, k)\u001b[0m\n\u001b[1;32m    361\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(population)\n\u001b[1;32m    362\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m k \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m n:\n\u001b[0;32m--> 363\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSample larger than population or is negative\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    364\u001b[0m result \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m k\n\u001b[1;32m    365\u001b[0m setsize \u001b[39m=\u001b[39m \u001b[39m21\u001b[39m        \u001b[39m# size of a small set minus size of an empty list\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wget\n",
    "import PyPDF2\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.agents import load_tools, Tool\n",
    "from langchain.utilities.google_search import GoogleSearchAPIWrapper\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from random import sample\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.chains import LLMRequestsChain, LLMChain\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "# First, let's load the language model we're going to use to control the agent.\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "\n",
    "google_search = GoogleSearchAPIWrapper()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_snippets_with_keywords(keyword: str, text: str, window_size: int = 500):\n",
    "    # Window size is in characters\n",
    "    all_keyword_indexes = [m.start() for m in re.finditer(f\"({keyword}).*[\\\\.]\", text)]\n",
    "    output = []\n",
    "    for index in all_keyword_indexes:\n",
    "        left_window = max(0, index - window_size)\n",
    "        right_window = min(len(text), index + window_size)\n",
    "        snippet = text[left_window:right_window]\n",
    "        output.append(snippet)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def get_keywords(query):\n",
    "    template = \"\"\"Select at most 3 most meaningful keywords in lowercase and separated by a comma from this text : {text}\"\"\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "    get_keywords_chain = LLMChain(prompt=prompt, llm=OpenAI(temperature=0), verbose=True)\n",
    "    \n",
    "    return [k.strip() for k in get_keywords_chain.run(query).split(\",\")]\n",
    "\n",
    "\n",
    "def google_search_about_education(query: str) -> str:\n",
    "    keywords = get_keywords(query)\n",
    "    print(keywords)\n",
    "\n",
    "    r = google_search._google_search_results(\n",
    "        f\"{query} filetype:pdf site:eduscol.education.fr\"\n",
    "    )\n",
    "    if len(r) == 0:\n",
    "        return \"No link\"\n",
    "    else:\n",
    "        all_docs = []\n",
    "        embeddings = HuggingFaceEmbeddings()\n",
    "        vectorstore = None\n",
    "        text_splitter = SpacyTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=2000, chunk_overlap=0, pipeline=\"fr_core_news_sm\"\n",
    "        )\n",
    "\n",
    "        all_relevant_snippets = []\n",
    "\n",
    "        for result in r[:3]:\n",
    "            pdf_url = result[\"link\"]  # Take the URL of the first result\n",
    "\n",
    "            # Download file\n",
    "            wget.download(url=pdf_url, out=\"data/result.pdf\")\n",
    "            document_text = \"\"\n",
    "            with open(\"data/result.pdf\", \"rb\") as pdf_file:\n",
    "                # creating a pdf reader object\n",
    "                pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "                for page in pdf_reader.pages:\n",
    "                    # extracting text from page\n",
    "                    document_text += page.extract_text()\n",
    "            os.remove(\"data/result.pdf\")\n",
    "\n",
    "            relevant_snippets = []\n",
    "\n",
    "            snippets_with_keyword = []\n",
    "            for keyword in keywords:\n",
    "                snippets_with_keyword = select_snippets_with_keywords(keyword, document_text, window_size=500)\n",
    "            snippets_with_keyword = text_splitter.split_text(\"\\n\".join(snippets_with_keyword))\n",
    "\n",
    "            min_snippets = 1\n",
    "            max_snippets = 3\n",
    "            max_vector_store = 50\n",
    "\n",
    "            if len(snippets_with_keyword) > max_snippets:\n",
    "                if len(snippets_with_keyword) > max_vector_store:\n",
    "                    snippets_with_keyword = sample(snippets_with_keyword, max_vector_store)\n",
    "                vectorstore = FAISS.from_texts(snippets_with_keyword, embeddings)\n",
    "                relevant_snippets = vectorstore.similarity_search(query, max_snippets)\n",
    "            if len(snippets_with_keyword) < min_snippets: \n",
    "                all_snippets = text_splitter.split_text(document_text)\n",
    "                random_snippets = sample(all_snippets, max_vector_store - len(snippets_with_keyword))\n",
    "                vectorstore = FAISS.from_texts(snippets_with_keyword, embeddings)\n",
    "                relevant_snippets = text_splitter.create_documents(snippets_with_keyword) \n",
    "                relevant_snippets += vectorstore.similarity_search(query, max_snippets - len(snippets_with_keyword))\n",
    "\n",
    "            all_relevant_snippets += relevant_snippets\n",
    "\n",
    "        # Select the most relevant snippets from the collection\n",
    "        vectorstore = FAISS.from_documents(all_relevant_snippets, embeddings)\n",
    "        docs = vectorstore.similarity_search(query, k=3)\n",
    "        # Reply to the question\n",
    "        chain = load_qa_chain(llm, chain_type=\"stuff\", verbose=True)\n",
    "        summary = chain.run({\"question\": query, \"docs\": docs})\n",
    "        return summary\n",
    "\n",
    "\n",
    "google_search_education = Tool(\n",
    "    \"Google Search about education\",\n",
    "    google_search_about_education,\n",
    "    \"A wrapper around Google Search, that returns only extracts of pdf about Education from the French government. Useful for when you need to answer questions about teaching. Input should be a short question in French.\",\n",
    ")\n",
    "\n",
    "tools = [google_search_education]\n",
    "\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "\n",
    "agent.run(\"quelle activité organiser pour favoriser apprentissage trandisciplinaire ?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out what autonomy means in CE1\n",
      "Action: Google Search about education\n",
      "Action Input: autonomie CE1\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 311, which is longer than the specified 200\n",
      "Created a chunk of size 208, which is longer than the specified 200\n",
      "Created a chunk of size 575, which is longer than the specified 200\n",
      "Created a chunk of size 339, which is longer than the specified 200\n",
      "Created a chunk of size 259, which is longer than the specified 200\n",
      "Created a chunk of size 506, which is longer than the specified 200\n",
      "Created a chunk of size 283, which is longer than the specified 200\n",
      "Created a chunk of size 236, which is longer than the specified 200\n",
      "Created a chunk of size 265, which is longer than the specified 200\n",
      "Created a chunk of size 210, which is longer than the specified 200\n",
      "Created a chunk of size 296, which is longer than the specified 200\n",
      "Created a chunk of size 244, which is longer than the specified 200\n",
      "Created a chunk of size 226, which is longer than the specified 200\n",
      "Created a chunk of size 285, which is longer than the specified 200\n",
      "Created a chunk of size 327, which is longer than the specified 200\n",
      "Created a chunk of size 220, which is longer than the specified 200\n",
      "Created a chunk of size 421, which is longer than the specified 200\n",
      "Created a chunk of size 224, which is longer than the specified 200\n",
      "Created a chunk of size 204, which is longer than the specified 200\n",
      "Created a chunk of size 402, which is longer than the specified 200\n",
      "Created a chunk of size 341, which is longer than the specified 200\n",
      "Created a chunk of size 297, which is longer than the specified 200\n",
      "Created a chunk of size 278, which is longer than the specified 200\n",
      "Created a chunk of size 249, which is longer than the specified 200\n",
      "Created a chunk of size 514, which is longer than the specified 200\n",
      "Created a chunk of size 209, which is longer than the specified 200\n",
      "Created a chunk of size 402, which is longer than the specified 200\n",
      "Created a chunk of size 233, which is longer than the specified 200\n",
      "Created a chunk of size 204, which is longer than the specified 200\n",
      "Created a chunk of size 203, which is longer than the specified 200\n",
      "Created a chunk of size 270, which is longer than the specified 200\n",
      "Created a chunk of size 239, which is longer than the specified 200\n",
      "Created a chunk of size 254, which is longer than the specified 200\n",
      "Created a chunk of size 322, which is longer than the specified 200\n",
      "Created a chunk of size 228, which is longer than the specified 200\n",
      "Created a chunk of size 331, which is longer than the specified 200\n",
      "Created a chunk of size 209, which is longer than the specified 200\n",
      "Created a chunk of size 283, which is longer than the specified 200\n",
      "Created a chunk of size 207, which is longer than the specified 200\n",
      "Created a chunk of size 292, which is longer than the specified 200\n",
      "Created a chunk of size 292, which is longer than the specified 200\n",
      "Created a chunk of size 317, which is longer than the specified 200\n",
      "Created a chunk of size 254, which is longer than the specified 200\n",
      "Created a chunk of size 209, which is longer than the specified 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mSi le travail en autonomie permet une meilleure prise en compte de l’hétérogénéité des élèves, il requiert \n",
      "la vigilance du professeur.\n",
      "\n",
      "Il n’est pas envisageable que des élèves de cours préparatoire soient livrés à eux-mêmes plus d’une quinzaine de minutes.\n",
      "\n",
      "Le professeur ne peut s’isoler avec un groupe pendant un trop long moment, laissant les autres élèves, inoccupés ou bloqués par les obstacles rencontrés.\n",
      "\n",
      "\n",
      "Certaines tâches ne peuvent être réalisées sans l’aide du professeur. | Les automatismes en écriture (copie, dictée, rédaction) s’obtiennent en privilégiant \n",
      "la répétition au  travers d’entraînements très réguliers, notamment lors d’activités \n",
      "ritualisées (dictée quotidienne, autodictée, phrase du jour, « jogging d’écriture »\n",
      "\n",
      ").\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Toute séance portant sur une nouvelle notion ne doit pas faire l’économie, en guise \n",
      "d’introduction, du brassage des acquis. | Le recours à cette \n",
      "classification doit devenir automatique pour amener les élèves à réfléchir sur leurs \n",
      "écrits. | -\n",
      "quence et la régularité de l’entraînement, avec la dictée quotidienne, garantissant \n",
      "l’acquisition d’automatismes.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "22 — Stanislas Dehaene, Apprendre !\n",
      "\n",
      "Les talents du cerveau, le défi \n",
      "des machines, Odile Jacob, 2018.\n",
      "98\n",
      "\n",
      "— | Or, faire l’économie de ce temps passé \n",
      "à apprendre à écrire prive l’élève de l’entraînement nécessaire à l’écriture en auto -\n",
      "nomie, d’autant qu’un entraînement régulier permet d’acquérir en peu de temps jus -\n",
      "tesse et rapidité. | outils d’aide variés (cahiers de référence, affichages didactiques).\n",
      "\n",
      "Le professeur s’assure que les élèves ont bien identifié la nature et les finalités de la tâche.\n",
      "\n",
      "Il s’assure que les consignes sont bien comprises ;\n",
      "\n",
      "il consacre le temps nécessaire au démarrage de l’activité.\n",
      "\n",
      "Il supervise l’avancée des travaux pour relancer l’intérêt et lever les obstacles.\n",
      "\n",
      "\n",
      "Le travail en autonomie requiert beaucoup de matériel, souvent construit par le professeur. | Aux activités de lecture, il faut ajouter la grammaire (45 minutes), \n",
      "12 — Lecture : construire le parcours d'un lecteur autonome, note \n",
      "de service du 25 avril 2018, Bulletin officiel spécial n° 3 du 26 avril 2018.\n",
      "\n",
      "\n",
      "42 — Quels supports et quelle méthode pour comprendre les textes ?\n",
      "les dictées (1 heure), l’écriture et la copie (1 heure), la  rédaction (30 minutes) et \n",
      "le vocabulaire (1 heure). \n",
      "\n",
      "\n",
      "LECTUREDURÉE  \n",
      "\n",
      "\n",
      "HEBDOMADAIRE | ●Construire\n",
      "\n",
      "dans une école un projet d’enseignement-apprentissage sur un même niveau de classe.32  V ademecum\n",
      "\n",
      "- Le pilotage des classes dédoublées\n",
      "\n",
      "- 100% de réussite en CP et CE1compétences acquises, au savoir-faire expérientiel.\n",
      "\n",
      "Elles permettent de s’adapter au milieu professionnel \n",
      "et constituent une intelligence pratique intuitive. | ●l’emploi du temps journalier et hebdomadaire, évolutif, offre une dimension encore plus rythmée et structurée aux apprentissages ;\n",
      " ●les conditions matérielles permettent de plus grandes adaptations (espace classe r epensé, jeux \n",
      "pédagogiques) sont au service des apprentissages ;\n",
      "\n",
      "\n",
      " ●l’apprentissage du travail en autonomie est renforcé, par une attention plus soutenue14  V ademecum\n",
      "\n",
      "- Le pilotage des classes dédoublées | Ces entraînements alternent néces -\n",
      "sairement des temps dirigés en présence du professeur et des temps de travail en \n",
      "autonomie, qu’ils soient individuels ou en groupe.\n",
      "\n",
      "Ils sont inscrits à l’emploi du temps.\n",
      "LIRE SANS ERREUR ET RAPIDEMENT DES MOTS COMPLEXES \n",
      "\n",
      "\n",
      "GRÂCE À UNE IDENTIFICATION MAÎTRISÉE\n",
      "\n",
      "\n",
      "Certaines activités aident particulièrement les élèves à identifier aisément et rapi -\n",
      "dement les mots en se détachant progressivement du décodage.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Pour permettre aux élèves de CE1 d'atteindre l'autonomie, le professeur doit leur offrir un environnement adapté et structuré, avec des activités variées et des entraînements réguliers qui alternent des temps dirigés et des temps de travail en autonomie. Des outils d'aide et des jeux pédagogiques peuvent également être mis en place pour faciliter l'apprentissage.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Pour permettre aux élèves de CE1 d'atteindre l'autonomie, le professeur doit leur offrir un environnement adapté et structuré, avec des activités variées et des entraînements réguliers qui alternent des temps dirigés et des temps de travail en autonomie. Des outils d'aide et des jeux pédagogiques peuvent également être mis en place pour faciliter l'apprentissage.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"autonomie en CE1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haxxor/projects/teacher/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 1.02k/1.02k [00:00<00:00, 630kB/s]\n",
      "Downloading:  38%|███▊      | 424M/1.11G [12:00<23:39, 485kB/s]     "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m----> 2\u001b[0m classifier \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39mzero-shot-classification\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m                       model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmorit/french_xlm_xnli\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/projects/teacher/lib/python3.8/site-packages/transformers/pipelines/__init__.py:724\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[39m# Infer the framework from the model\u001b[39;00m\n\u001b[1;32m    721\u001b[0m \u001b[39m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[39m# Will load the correct model if possible\u001b[39;00m\n\u001b[1;32m    723\u001b[0m model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[0;32m--> 724\u001b[0m framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[1;32m    725\u001b[0m     model,\n\u001b[1;32m    726\u001b[0m     model_classes\u001b[39m=\u001b[39;49mmodel_classes,\n\u001b[1;32m    727\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    728\u001b[0m     framework\u001b[39m=\u001b[39;49mframework,\n\u001b[1;32m    729\u001b[0m     task\u001b[39m=\u001b[39;49mtask,\n\u001b[1;32m    730\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    731\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m    732\u001b[0m )\n\u001b[1;32m    734\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    735\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/projects/teacher/lib/python3.8/site-packages/transformers/pipelines/base.py:257\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    252\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTrying to load the model with Tensorflow.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m     )\n\u001b[1;32m    256\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 257\u001b[0m     model \u001b[39m=\u001b[39m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    258\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39meval\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    259\u001b[0m         model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/projects/teacher/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:463\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    462\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    464\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    465\u001b[0m     )\n\u001b[1;32m    466\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    467\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    469\u001b[0m )\n",
      "File \u001b[0;32m~/projects/teacher/lib/python3.8/site-packages/transformers/modeling_utils.py:2137\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2122\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2123\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   2124\u001b[0m     cached_file_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m   2125\u001b[0m         cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[1;32m   2126\u001b[0m         force_download\u001b[39m=\u001b[39mforce_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2135\u001b[0m         _commit_hash\u001b[39m=\u001b[39mcommit_hash,\n\u001b[1;32m   2136\u001b[0m     )\n\u001b[0;32m-> 2137\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcached_file_kwargs)\n\u001b[1;32m   2139\u001b[0m     \u001b[39m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   2140\u001b[0m     \u001b[39m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   2141\u001b[0m     \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m SAFE_WEIGHTS_NAME:\n\u001b[1;32m   2142\u001b[0m         \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/teacher/lib/python3.8/site-packages/transformers/utils/hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    406\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    410\u001b[0m         path_or_repo_id,\n\u001b[1;32m    411\u001b[0m         filename,\n\u001b[1;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    421\u001b[0m     )\n\u001b[1;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    425\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/teacher/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:124\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    120\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(\n\u001b[1;32m    121\u001b[0m         fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[1;32m    122\u001b[0m     )\n\u001b[0;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/projects/teacher/lib/python3.8/site-packages/huggingface_hub/file_download.py:1242\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[1;32m   1240\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mdownloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, temp_file\u001b[39m.\u001b[39mname)\n\u001b[0;32m-> 1242\u001b[0m     http_get(\n\u001b[1;32m   1243\u001b[0m         url_to_download,\n\u001b[1;32m   1244\u001b[0m         temp_file,\n\u001b[1;32m   1245\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1246\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m   1247\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1248\u001b[0m     )\n\u001b[1;32m   1250\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mstoring \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, blob_path)\n\u001b[1;32m   1251\u001b[0m _chmod_and_replace(temp_file\u001b[39m.\u001b[39mname, blob_path)\n",
      "File \u001b[0;32m~/projects/teacher/lib/python3.8/site-packages/huggingface_hub/file_download.py:495\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries)\u001b[0m\n\u001b[1;32m    486\u001b[0m total \u001b[39m=\u001b[39m resume_size \u001b[39m+\u001b[39m \u001b[39mint\u001b[39m(content_length) \u001b[39mif\u001b[39;00m content_length \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    487\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[1;32m    488\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    489\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    493\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[1;32m    494\u001b[0m )\n\u001b[0;32m--> 495\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m):\n\u001b[1;32m    496\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    497\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/projects/teacher/lib/python3.8/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/projects/teacher/lib/python3.8/site-packages/urllib3/response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[0;32m--> 628\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    630\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m    631\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/projects/teacher/lib/python3.8/site-packages/urllib3/response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    564\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    566\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 567\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/teacher/lib/python3.8/site-packages/urllib3/response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:459\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 459\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    461\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:503\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    498\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[1;32m    500\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 503\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[1;32m    505\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    506\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.8/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:  38%|███▊      | 425M/1.11G [12:17<23:39, 485kB/s]"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=100, chunk_overlap=0)\n",
    "texts = text_splitter.split_text(state_of_the_union)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('teacher')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b0be345df10520cdb2ac4dcd9a10cc2d65672523ffae5755a14eab5a8e26bee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
